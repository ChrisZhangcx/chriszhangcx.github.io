<!DOCTYPE html>




<html class="theme-next gemini" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP,">










<meta name="description" content="Attention Is All You Need1. 预备知识1.1 Encoder/Decoder序列模型由Encoder与Decoder两部分构成，简单来说，Encoder对input进行编码，而Decoder对编码结果进行解码，解码的结果就是整个序列模型的输出。借用Andrew Ng课件中的截图来进行说明：  Encoder的每一个单元会根据前一个hidden state以及当前的inpu">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Crash Paper: Attention Is All You Need">
<meta property="og:url" content="/Crash-Paper-Attention-Is-All-You-Need/index.html">
<meta property="og:site_name" content="Chris Zhang">
<meta property="og:description" content="Attention Is All You Need1. 预备知识1.1 Encoder/Decoder序列模型由Encoder与Decoder两部分构成，简单来说，Encoder对input进行编码，而Decoder对编码结果进行解码，解码的结果就是整个序列模型的输出。借用Andrew Ng课件中的截图来进行说明：  Encoder的每一个单元会根据前一个hidden state以及当前的inpu">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="/Crash-Paper-Attention-Is-All-You-Need/001.png">
<meta property="og:image" content="/Crash-Paper-Attention-Is-All-You-Need/002.png">
<meta property="og:image" content="/Crash-Paper-Attention-Is-All-You-Need/101.png">
<meta property="og:updated_time" content="2018-12-30T05:25:37.324Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Crash Paper: Attention Is All You Need">
<meta name="twitter:description" content="Attention Is All You Need1. 预备知识1.1 Encoder/Decoder序列模型由Encoder与Decoder两部分构成，简单来说，Encoder对input进行编码，而Decoder对编码结果进行解码，解码的结果就是整个序列模型的输出。借用Andrew Ng课件中的截图来进行说明：  Encoder的每一个单元会根据前一个hidden state以及当前的inpu">
<meta name="twitter:image" content="/Crash-Paper-Attention-Is-All-You-Need/001.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="/Crash-Paper-Attention-Is-All-You-Need/">





  <title>Crash Paper: Attention Is All You Need | Chris Zhang</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chris Zhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="/Crash-Paper-Attention-Is-All-You-Need/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chengxi Zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chris Zhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Crash Paper: Attention Is All You Need</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表时间</span>
              
              <time title="创建时间" itemprop="dateCreated datePublished" datetime="2018-12-26T19:56:09+08:00">
                2018-12-26
              </time>
            

            
            <span class="post-updated">
              &nbsp; | &nbsp; 
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              更新时间

              <time itemprop="dateUpdated" datetime="2018-12-30T13:25:37+08:00" content="2018-12-30">
                2018-12-30
              </time>
            </span>
            

            
          </span>
          

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h1><h2 id="1-预备知识"><a href="#1-预备知识" class="headerlink" title="1. 预备知识"></a>1. 预备知识</h2><h3 id="1-1-Encoder-Decoder"><a href="#1-1-Encoder-Decoder" class="headerlink" title="1.1 Encoder/Decoder"></a>1.1 Encoder/Decoder</h3><p>序列模型由Encoder与Decoder两部分构成，简单来说，Encoder对input进行编码，而Decoder对编码结果进行解码，解码的结果就是整个序列模型的输出。借用Andrew Ng课件中的截图来进行说明：</p>
<p><img src="/Crash-Paper-Attention-Is-All-You-Need/001.png" alt=""></p>
<p>Encoder的每一个单元会根据前一个hidden state以及当前的input来计算当前的hidden state，并将其传到下一个单元中去。而Decoder对于传入的单元进行解码，来找到最合适的一个输出。注意每个Decoder单元只会输出一个output symbol。</p>
<a id="more"></a>
<p>以machine translation问题为例给出具体说明。machine translation问题其实是在寻找一个条件概率，在给定输入x（待翻译句子）的前提下寻找结果概率最大，作为输出的翻译结果，具体步骤可以描述如下：</p>
<ol>
<li>Decoder的第一个单元对于Encoder部分输出的最后一个hidden state(我们把它记为$h^0$)，在候选词表中找一个词A，使得$P(A|x)$在整个词汇表中最大。</li>
<li>在第二个单元中，使用$h^1$以及第一单元的输出A作为输入，我们继续找一个单词使$P(A, B|x)$最大，由于$P(A, B|x)=P(B|A, X)P(A|X)$，我们又在第一单元中记录了$P(A|X)$大小，实际上就是对每个单词B，计算$P(B|A, X)$，然后使得$P(B|A, X)P(A|X)$最大即可。</li>
<li>同理，一直向下逐个搜索单词，直到出现所有的词输出Prob都不如当前的Prob，就加入EOS并输出。</li>
</ol>
<h3 id="1-2-Beam-Search"><a href="#1-2-Beam-Search" class="headerlink" title="1.2 Beam Search"></a>1.2 Beam Search</h3><p>有了序列模型的基础，<code>Beam Search实际上就是序列模型选择输出哪一个symbol的方式</code>。</p>
<p>正常来说，我们只需要按照上面的算法不停的算最大概率，每次选择最好的输出就可以了，但是实际上，如果待翻译的语法表中有1000个单词，待翻译的句子长度是10，那么可能的句子构成数量就是$1000^{10}$个。所以直接的全局搜索是不可取的，我们需要找一个能获得近似最优解的词序列，但是又要有能够接受的时间复杂度，<code>Beam Search</code>（集束搜索）应运而生。</p>
<p>集束搜索和贪心其实有一定的相似之处：如果我们选择贪心算法来解这个问题，在搜索最优词时只考虑概率最大的一个候选单词，我们很有可能错过最优解以及很多很好的解。而Beam Search有一个参数 <code>beam width</code> ，也就是每次选择的概率最大的候选单词数，每次使用前面选定的单词组继续向后扩展一个单词，再从其中所有的概率里挑出最有可能的top n。</p>
<p>由于每次算得的Prob都很小，当输出序列比较长的时候，就会出现数值下溢的问题（这也会导致了beam search倾向于寻找更短的答案）。为了解决这个问题，我们要使用 <code>Length Normalization</code> ：将目标函数改为Prob的对数和，而不是取Prob的乘积；另外，我们要添加一个正则项，也就是算得对数和之后，我们可以求它的平均值，也就是除以输出的单词数量（<em>在实践中我们通常会使用数量的0.7次方来进行正则</em>）。</p>
<p>我们已经知道，Beam Search的参数有：束宽，单词数的惩罚系数$\alpha$。直观上来说，我们可以知道束宽越大，结果会越好，但是占用的计算时间会更长，内存也会更高。 <code>现在，还有这样一个值得考虑的问题</code> ：如果我们的输出结果不是我们人工所预期的，我们如何知道是RNN——encoder/decoder模型导致了这个结果，还是集束搜索的beam width设置的不够大，没有搜到真正的结果呢？</p>
<p>一个最简单的判断办法是：我们把人工所预期的结果放到RNN模型的decoder中，计算$P(y^*|x)$，如果这个Prob大于模型本身所预测的结果，说明是集束搜索错过了这个更好的结果，而如果Prob要比原来错误结果的Prob更小，那么很显然就是RNN模型出了问题。</p>
<h3 id="1-3-Attention"><a href="#1-3-Attention" class="headerlink" title="1.3 Attention"></a>1.3 Attention</h3><p>Attention-中文称注意力机制，这个概念的提出是为了解决：<strong>神经网络对记忆长句子的能力较弱，随着待翻译的句子长度增加，其BLEU值将会逐渐下滑</strong>这个问题。</p>
<p>我们可以这样来直观的理解Attention机制到底是什么：Attention将决定模型在输出第$t$个output symbol时，<code>how much attention should be paid to t&#39;th input symbol</code>。Attention的具体计算思路如下：</p>
<p>对于每个input symbol，使用一个双向RNN（GRU/LSTM, usually LSTM）来提取每个symbol的特征。因为RNN是双向的，所以第$t$个input symbol的特征$a^t$将由前面传来的$\overrightarrow{a}^t$与后面传来的$\overleftarrow{a}^t$共同决定（Andrew Ng的课件中没有说怎么通过后两者来计算$a^t$，个人觉得就是简单的sum或者concatenate吧）。为了方便表述，我们记$a^t=&lt;\overrightarrow{a}^t,\overleftarrow{a}^t&gt;$</p>
<p>这样，每个input symbol都会有一个描述它自身的特征向量$a^t$。类比于一个全连接层，我们同样将特征向量映射到所有的Decoder单元中，其中第$t^*$个待翻译单词到第$t$个翻译后的输出单词的连接对应的权重为$\alpha^{&lt; t^*, t&gt;}$，而所有连接到第$t$个单词的待翻译单词加权和共同构成了上下文$C$对于这个输出单词的权重，在此记作$C^t$。同样使用Andrew Ng的课件截图来对这个过程进行总结 <del>我似乎已经暴露了这些东西都是在Andrew Ng教授的课件中学的了？</del></p>
<p><img src="/Crash-Paper-Attention-Is-All-You-Need/002.png" alt=""></p>
<p>现在的问题转变成了：我们如何确定第$t^*$个待翻译单词到第$t$个翻译后的输出单词的连接对应的权重值$\alpha^{&lt; t^*, t&gt;}$呢？</p>
<p>以第一个output symbol为例，需要计算所有input symbol到它的attention加权之和，我们依次把n个input对于第一个output的权值标记为$\alpha^{&lt; 1, 1&gt;}, \alpha^{&lt; 1, 2&gt;}, …, \alpha^{&lt; 1, n&gt;}$，我们有以下两个要求：</p>
<ol>
<li>所有的权值和为1，也就是要有归一化。</li>
<li>因为计算第i个input权值时，我们只知道第i-1个hidden state: $h^{i-1}$与$\alpha^{<1,i-1>}$，所以直观地，应该有$\alpha^{<1,i>}=f(h^{i-1}, \alpha^{<1,i-1>})$。</1,i-1></1,i></1,i-1></li>
</ol>
<p>第一个条件很好满足，假设我们已经计算出了每个input对应的权重，对它使用softmax就可以使所有的权值和为1。</p>
<p>要满足第二个条件其实也比较简单：我们训练一个单层的简单网络，这个网络的输入是$a^t=&lt;\overrightarrow{a}^t,\overleftarrow{a}^t&gt;$与input symbol$_t$，输出是$e<1,t>$，也就是第t个input symbol对于待翻译词的权重 <del>没错，权重就这么被训练出来了</del> 。但是Andrew课件中并没有说这个监督学习的训练对怎么出来，难道是要人工标注？还是涉及到了什么<del>我没有学过所以显得</del>深不可测的知识？</1,t></p>
<p>总之，这样我们就可以计算整个上下文对于某个输出的影响了，从概念上来理解，就相当于每个输出在学习前一个hidden state与前一个输出的前提下，又学习了整个上下文与它的关系，因为上下文中每个symbol和它的权值都不同，所以某些和它关系比较大的symbol权重就会大，也就是我们常理解的output symbol pay attention to input symbol了。OVER。</p>
<h2 id="2-Attention-Is-All-You-Need"><a href="#2-Attention-Is-All-You-Need" class="headerlink" title="2. Attention Is All You Need"></a>2. Attention Is All You Need</h2><h3 id="2-1-软广"><a href="#2-1-软广" class="headerlink" title="2.1 软广"></a>2.1 软广</h3><p>先推荐一个理解论文时候发现的很有意思的博客：<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">jalammar大神对transformer的图形化解释</a>。这个博客会用很生动的例子与大量动图来解释一些模型，讲的很透彻也很浅显易懂，<del>就是更新慢了点</del>。</p>
<p><code>下面关于模型讲解的内容中，有相当的一部分来自于这个博客。</code></p>
<h3 id="2-2-Transformer"><a href="#2-2-Transformer" class="headerlink" title="2.2 Transformer"></a>2.2 Transformer</h3><p>我们先来从宏观角度来看，Google Brain的大牛们提出了什么样的一个模型。在我看来，其主要特性是这样的：</p>
<ol>
<li>你们看，我们真的没有用任何CNN和RNN模型哦！（指着Transformer）</li>
<li>我们真的没有……偷偷加入multi-head <del>真香😋</del> </li>
<li>我们真的……偷偷加入position encoding <del>真香😋</del></li>
</ol>
<p>好了，认真来说，论文的模型最重要的特性就是<code>完全没有用到RNN/CNN模型</code>。但是不代表模型完全没有使用到这两个模型的思维模式：CNN对特征的高维抽取，在Transformer中用multi-head attention里面的多个产生Query/Key/Value的矩阵替代了，而RNN模型中对序列进行建模的特点，也在Transformer中使用了position encoding来进行变相的替代。<em>我个人挺惊讶于这种把一堆vector用做sum了之后的最终vector放到模型里面跑最终真的非常work的这个结果</em>，可能是大牛们对这些特征的捕捉真的非常到位吧。可以说是非常佩服了。</p>
<p>还有一个很有意思的地方：大牛们对attention这个机制从另一个层面做了解读。上面我们已经说了，attention其实就是在给出输出时，该输出需要参照多少信息在某个input symbol中，而🐂们的理解方式是：我目前要输出的output symbol是一个 <code>query</code> ，而每个input symbol是一个 <code>key</code> ，这个key有对应的 <code>value</code> ，也就是上面所提到的，这个input symbol被双向RNN提取出来的特征。当前output对所有input symbol进行attention的过程，就好比是 <code>用这个query来对所有的key进行模糊查询，获得这个key的value</code> 。模糊查询有多<em>模糊</em>呢，就是每个input symbol对于当前output的权重啦。上面已经说到，这个权重是训练之后使用softmax归一得到的。</p>
<p><del>希望你也觉得上面的东西很有意思😄</del> 接下来对模型结构进行详细说明。</p>
<h4 id="2-2-1-Encoding-Decoding模块"><a href="#2-2-1-Encoding-Decoding模块" class="headerlink" title="2.2.1 Encoding/Decoding模块"></a>2.2.1 Encoding/Decoding模块</h4><p>整个Transformer模型 <code>就是一个Encoder与一个Decoder</code> ，还是以machine translation为例：首先把句子输入到Encoder中，这个Encoder会使用self-attention（简单起见，可以把论文里的scaled dot-product attention/multi-head attention都先看成self-attention）来计算每个input symbol的输出，然后弄成一个vector（因为一个input symbol会产生多个输出——当然不是对所有的vector进行concatenate这么简单，后面 <del>可能</del> 会进行说明）。然后把它们喂给一个前向传播网络，如是反复。Encoder的最终输出结果被转换成两个矩阵<strong>K与V</strong>，分别对应Key与Value。</p>
<p>Decoder从零向量开始逐个产生output symbol的输出，首先对output sequence同样进行self-attention，不过这次有一个改变的地方，论文中称此为 <code>Masked Multi-Head Attention</code> ，在前面的attention过程中，每个query是知道所有的key的信息做加权的，然而在输出的过程中，对应位置的query只能知道它及之前的位置信息，确保对位置i的预测只能依赖小于i的位置处的已知输出。</p>
<h4 id="2-2-2-Scaled-Dot-Product-Attention-Multi-Head-Attention"><a href="#2-2-2-Scaled-Dot-Product-Attention-Multi-Head-Attention" class="headerlink" title="2.2.2 Scaled Dot-Product Attention/Multi-Head Attention"></a>2.2.2 Scaled Dot-Product Attention/Multi-Head Attention</h4><p>在上面，我们知道了什么是attention，实际上是对input symbol的特征进行加权，然后输入到Decoder单元中。在Transformer中进行的处理是类似的，其中使用了一个叫 <code>self-attention</code> 的attention方法，顾名思义，就是自己对自己进行attention，比如有个句子：</p>
<blockquote>
<p>The animal didn’t cross the street because it was too tired</p>
</blockquote>
<p>在Encoder模块中，这个句子中的每个word，都会与其它所有词进行attention，这个attention连接的权重，或者说Query与Key的模糊匹配程度，揭示了两个词之间的相关程度。比如，可能对词 <code>it</code> 进行self-attention时，发现 <code>animal</code> 与它的连接权重很大，就能一定程度上揭示这个代词指代的是animal啦 <del>因为作为一个没用的模型，我一开始是不知道这个it是指代animal还是street的呀</del> 。</p>
<p>具体来看怎么进行self-attention。如下图所示，比如现在有一个词 <code>Thinking</code> ，已经对它做了embedding，它是一个$d=512$维的向量$X_1$。我们首先把它变成一个 <code>查询-Query</code> 。虽然说是query，其实它还是一个vector，那么怎么把它变成query呢，我们把它乘以一个转换矩阵$W^Q$，这样它就由$X_1$变成了$q_1$。</p>
<p><img src="/Crash-Paper-Attention-Is-All-You-Need/101.png" alt=""></p>
<p>同样的道理，我们把它乘以转换$W^K$，把它变成一个 <code>Key</code> ，把它乘以转换矩阵$W^V$，把它变成一个 <code>Value</code>。这么一看，$X_1$的作用其实很大，所有的一切都是由它自己延伸出来的——这么说对$W^Q,W^K,W^V$矩阵三兄弟就太不公平了，它们仨承担了转换$X_1$的主要责任，它们是怎么来的呢？ <code>它们是在训练过程中训练出来的</code> 。<strong>我其实很好奇它们是怎么训练出来的，毕竟论文里面没有详细说</strong>。</p>
<p>现在我们有了 <code>queries, keys, values</code> ，剩下的就很简单了，我们求$q_1$与所有的$k$的点积，然后用softmax归一，这就是self-attention中其它词对thinking这个词的权重。对应的value乘以对应的weight，然后加权就是 <code>X1所获得的上下文信息C</code> 。</p>
<p>知道了什么是self-attention，现在可以很简单的解释标题中的两个attention到底是什么了：</p>
<ol>
<li>Scaled Dot-Product Attention：实际就是上面所说的加权的整个过程，Dot-Product意为点积，就是指Queries与Keys做点积；Scaled是指在工程实践中，发现在较大的维度情况下，点积会大幅度增大，由此softmax容易陷入局部极小，所以要除以维度的开方，即$\sqrt{d_k}$，这就是scaled词的由来了。这个attention与self-attention的差别仅在于 <code>在softmax之前使用key的维度进行了scale</code> 。</li>
<li>Multi-Head Attention：1中的Attention我们做一次是不够的，可能挖不出来词与词之间的关联关系（或者说挖不完全？），所以上面的$W^Q,W^K,W^V$，我们要生成多个（论文中使用的是8个），这样一个word就会输出多个vector了——可是我们最终是要一个word只有一个vector呀，没关系，我们先把所有的输出concatenate起来，最后再使用一个转换矩阵$W^O$把它转换到正常的维度，这样我们就在多个层次度量了词与词之间的语义关系，并且最终变成了我们想要的输出，这就是命名为Multi-Head的原因。那么你可能又要问了，这个$W^O$又是怎么来的呢，没错，<code>它又是训练出来的</code>，而且我又不知道它具体是怎么训练的，可以说是非常伤心了。</li>
</ol>
<h4 id="2-2-3-Position-Encoding"><a href="#2-2-3-Position-Encoding" class="headerlink" title="2.2.3 Position Encoding"></a>2.2.3 Position Encoding</h4><p>如果对RNN稍有了解你可能会发现，上面的所有Attention过程都是对词与词之间进行关联的计算，我们如果直接把上面的结果放进去跑，似乎错过了一个很重要的信息：序列的顺序。</p>
<p>所以呢， <code>在进行attention之前，也就是刚做完Word Embedding的时候，我们要在Embedding中加入每个词的位置信息</code> 。</p>
<script type="math/tex; mode=display">
PE(pos,2i)=sin(pos/10000^{2i/d_model})</script><script type="math/tex; mode=display">
PE(pos,2i+1)=cos(pos/10000^{2i/d_model})</script><p>👆上面就是论文中给出的获得Position Embedding的方法。我们把这个Embedding加上最开始获得的词向量，就是最开始的input embedding了。</p>
<h3 id="2-3-模型结构概括"><a href="#2-3-模型结构概括" class="headerlink" title="2.3 模型结构概括"></a>2.3 模型结构概括</h3><p>简单总结一下Transformer的工作流程：</p>
<ol>
<li>首先计算每个词的embedding，这是一切的基础。</li>
<li>在输入一句话时，每个词都有自己存在的顺序下标，使用2.2.3中描述的公式进行这句话每个词的Position Encoding，并加到1中算得的embedding中去。这一步要在输入数据的时候执行，因为只有一句话给出的时候，我们才能知道每个词在这句话的什么位置。<br><code>接下来进入Encode步骤</code></li>
<li>使用Multi-Head Attention计算每个词关于其他词的关联关系，并合成一个最终向量（使用2.2.2中说的$W^O$），最终输出的一句话中，每个词都会是一个vector。<code>因为这个过程中，对每一个word的处理都是独立的，所以可以并行化执行。这是Transformer极其重要的优势之一。</code></li>
<li>把3中输出的一句话中的所有vector送到一个前馈全连接网络，网络的参数在训练中确定，继续输出所有变换后的vector。</li>
<li>Encoder中重复了3、4两个步骤一共6次，最顶层的Encoder单元将输出的vectors转换成 <del>这里没有说怎么转换的</del> 矩阵<strong>K与V</strong>。这两个矩阵是用来计算Encoder-Decoder Attention的。<br><code>接下来进入Decode步骤</code></li>
<li>在Decoder的第一个sublayer中，对输入的output向量做Multi-Head Attention，原理同2.2.2。</li>
<li>第一个sublayer的输出对K/V矩阵再做一次Encoder-Decoder Attenion：使用输出vector用K/V矩阵计算加权。这是第二个sublayer。</li>
<li>第三个sublayer中，使用前馈全连接网络对vector进行变换。</li>
<li>Decoder中重复了6、7、8三个步骤一共6次，最顶层的Decoder单元每次会输出一个vector，使用一个简单的全连接网络把这个向量映射到整个输出空间vocabulary表中，来获得每个vocabulary的概率，并使用Softmax进行归一。</li>
<li>选择9中概率最大的word作为本次Decoder的输出单词。</li>
<li>重复9、10两步直到最后一个输出的word是特殊字符：EOS，表示一个字符串的结束。</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Leetcode-131-140解题笔记/" rel="next" title="Leetcode: 131-140解题笔记">
                <i class="fa fa-chevron-left"></i> Leetcode: 131-140解题笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Leetcode-解题笔记索引/" rel="prev" title="Leetcode 解题笔记索引">
                Leetcode 解题笔记索引 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chengxi Zhang</p>
              <p class="site-description motion-element" itemprop="description">Is life always this hard, or is it just when you're a kid?</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/chriszhangcx" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:chengxi_zhang@pku.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-Is-All-You-Need"><span class="nav-number">1.</span> <span class="nav-text">Attention Is All You Need</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-预备知识"><span class="nav-number">1.1.</span> <span class="nav-text">1. 预备知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Encoder-Decoder"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 Encoder/Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Beam-Search"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 Beam Search</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Attention"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 Attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Attention-Is-All-You-Need"><span class="nav-number">1.2.</span> <span class="nav-text">2. Attention Is All You Need</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-软广"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 软广</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Transformer"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-Encoding-Decoding模块"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">2.2.1 Encoding/Decoding模块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-Scaled-Dot-Product-Attention-Multi-Head-Attention"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">2.2.2 Scaled Dot-Product Attention/Multi-Head Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-Position-Encoding"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">2.2.3 Position Encoding</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-模型结构概括"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 模型结构概括</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-snowflake-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chengxi Zhang</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  













  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

<script src="/lib/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"lib/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/lib/assets/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body>
</html>
